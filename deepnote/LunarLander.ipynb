{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Q-Learning on LunarLander-v2 Environment",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 5,
          "w": 12,
          "x": 0,
          "y": 1121
        },
        "cell_id": "369aba67b2694422a1a7b4e81dac984b",
        "deepnote_cell_type": "text-cell-h1"
      }
    },
    {
      "cell_type": "markdown",
      "source": "Author: Gleb Tcivie",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "underline": true
            },
            "toCodePoint": 19,
            "fromCodePoint": 0
          }
        ],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 0
        },
        "cell_id": "cea78b9b6a0346579770fad9f9231f6a",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "In this project, we are training a Q-Learning agent to solve the `LunarLander-v2` environment from OpenAI's Gym.",
      "metadata": {
        "formattedRanges": [
          {
            "url": "https://gymnasium.farama.org/environments/box2d/lunar_lander/",
            "type": "link",
            "ranges": [],
            "toCodePoint": 112,
            "fromCodePoint": 65
          }
        ],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1127
        },
        "cell_id": "14b03a7c5ab44a92a3f58833f918ac2f",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "The objective of the LunarLander-v2 environment is to optimize the trajectory of a spacecraft landing on the moon. The environment is modeled after the classic rocket trajectory optimization problem, with the actions being discrete in nature - either fire the engine at full throttle or keep it off.",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1130
        },
        "cell_id": "f66a5eb9d27c40459b3c237bd2fc3b6c",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "If you would like to just straight ahead and view this experiment's results you can see it here.",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "bold": true
            },
            "toCodePoint": 84,
            "fromCodePoint": 0
          },
          {
            "url": "https://api.wandb.ai/links/got-tree/0tw2h06o",
            "type": "link",
            "ranges": [
              {
                "type": "marks",
                "marks": {
                  "bold": true
                },
                "toCodePoint": 11,
                "fromCodePoint": 0
              }
            ],
            "toCodePoint": 95,
            "fromCodePoint": 84
          },
          {
            "type": "marks",
            "marks": {
              "bold": true
            },
            "toCodePoint": 96,
            "fromCodePoint": 95
          }
        ],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1130
        },
        "cell_id": "f2355ad48d554cbe82a797a13c2de572",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Training Process",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 5,
          "w": 12,
          "x": 0,
          "y": 1133
        },
        "cell_id": "578d426f116c4b5297937dc26a29b8a1",
        "deepnote_cell_type": "text-cell-h2"
      }
    },
    {
      "cell_type": "markdown",
      "source": "For the LunarLander problem, I adopted a similar approach as with the MountainCar environment and implemented a Q-Learning algorithm. However, this time I expanded upon the exploration strategy by introducing an Upper Confidence Bound (UCB) policy. My objective was to compare the performance and characteristics of the UCB policy against the ε-greedy policy, a commonly used method in Q-Learning. In addition, I aimed to evaluate the effects of applying an optimistic initialization strategy in combination with the ε-greedy policy.",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1139
        },
        "cell_id": "7854a9bb10fd41d782e6fd09b1a122b4",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "The UCB policy is designed to balance exploration and exploitation in reinforcement learning. It leverages uncertainty and variance in the reward estimates to guide the exploration process. This differs from the ε-greedy policy, which explores the action space purely randomly with a certain probability ε.",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1142
        },
        "cell_id": "c8b16c6ddd074f3280e6f8fdfd864241",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "On the other hand, optimistic initialization is a simple yet powerful method to encourage exploration in the early stages of training. By initializing the Q-values optimistically (i.e., with higher than achievable values), the agent is incentivized to explore all actions to learn their actual rewards.",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1145
        },
        "cell_id": "8c3dd69a53e14666b432800ee43003e2",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Hyper Parameter Tuning",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 5,
          "w": 12,
          "x": 0,
          "y": 1148
        },
        "cell_id": "bcff3cb65789428697300e80d3889c02",
        "deepnote_cell_type": "text-cell-h2"
      }
    },
    {
      "cell_type": "markdown",
      "source": "We use Weights & Biases Sweeps for hyperparameter tuning. We explore different values of the learning rate, discount factor, and the number of discretized states. The agent's performance is measured by the average reward over 100 episodes.",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1154
        },
        "cell_id": "fd63f5da5eac4fc6bd563ff2b97f946d",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "# Code and Running",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 5,
          "w": 12,
          "x": 0,
          "y": 1157
        },
        "cell_id": "898255adeb8d4c2db29e21d09f6b491c",
        "deepnote_cell_type": "text-cell-h1"
      }
    },
    {
      "cell_type": "markdown",
      "source": "Imports and Installs",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1163
        },
        "cell_id": "72d2e4f208544f4685e509d41610c094",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "code",
      "source": "!pip install swig\n!pip install wandb\n!pip install gym[all]\n!pip install imageio",
      "metadata": {
        "source_hash": "72ea421b",
        "execution_start": 1690103301647,
        "execution_millis": 228492,
        "deepnote_app_coordinates": {
          "h": 32,
          "w": 12,
          "x": 0,
          "y": 1166
        },
        "deepnote_to_be_reexecuted": false,
        "cell_id": "3f8bce9137504debb68552aca3b3a964",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: swig in /root/venv/lib/python3.9/site-packages (4.1.1)\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mCollecting wandb\n  Downloading wandb-0.15.5-py3-none-any.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting GitPython!=3.1.29,>=1.0.0\n  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 KB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from wandb) (2.28.1)\nCollecting PyYAML\n  Downloading PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m738.9/738.9 KB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from wandb) (3.19.6)\nCollecting docker-pycreds>=0.4.0\n  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\nCollecting setproctitle\n  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\nRequirement already satisfied: setuptools in /root/venv/lib/python3.9/site-packages (from wandb) (58.1.0)\nRequirement already satisfied: typing-extensions in /shared-libs/python3.9/py/lib/python3.9/site-packages (from wandb) (4.4.0)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from wandb) (1.24.0)\nCollecting appdirs>=1.4.3\n  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\nRequirement already satisfied: psutil>=5.0.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from wandb) (5.9.3)\nCollecting pathtools\n  Downloading pathtools-0.1.2.tar.gz (11 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from wandb) (8.1.3)\nRequirement already satisfied: six>=1.4.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nCollecting gitdb<5,>=4.0.1\n  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nCollecting smmap<6,>=3.0.1\n  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\nBuilding wheels for collected packages: pathtools\n  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8808 sha256=cee9200b5cc9c0755e2633a6a96e1435243eccd6f0ea3412feca3815a2a1e7ce\n  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\nSuccessfully built pathtools\nInstalling collected packages: pathtools, appdirs, smmap, setproctitle, PyYAML, docker-pycreds, gitdb, GitPython, wandb\nSuccessfully installed GitPython-3.1.32 PyYAML-6.0.1 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.5\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mCollecting gym[all]\n  Downloading gym-0.26.2.tar.gz (721 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 KB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from gym[all]) (1.23.4)\nCollecting gym-notices>=0.0.4\n  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\nRequirement already satisfied: importlib-metadata>=4.8.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from gym[all]) (5.0.0)\nCollecting cloudpickle>=1.2.0\n  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\nRequirement already satisfied: swig==4.* in /root/venv/lib/python3.9/site-packages (from gym[all]) (4.1.1)\nRequirement already satisfied: matplotlib>=3.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from gym[all]) (3.6.0)\nCollecting ale-py~=0.8.0\n  Downloading ale_py-0.8.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pytest==7.0.1\n  Downloading pytest-7.0.1-py3-none-any.whl (296 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.0/297.0 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting mujoco-py<2.2,>=2.1\n  Downloading mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting imageio>=2.14.1\n  Downloading imageio-2.31.1-py3-none-any.whl (313 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 KB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting moviepy>=1.0.0\n  Downloading moviepy-1.0.3.tar.gz (388 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.3/388.3 KB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting opencv-python>=3.0\n  Downloading opencv_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting lz4>=3.1.0\n  Downloading lz4-4.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting mujoco==2.2\n  Downloading mujoco-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting box2d-py==2.3.5\n  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 KB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting pygame==2.1.0\n  Downloading pygame-2.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting glfw\n  Downloading glfw-2.6.2-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (208 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.2/208.2 KB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: absl-py in /shared-libs/python3.9/py/lib/python3.9/site-packages (from mujoco==2.2->gym[all]) (1.3.0)\nCollecting pyopengl\n  Downloading PyOpenGL-3.1.7-py3-none-any.whl (2.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tomli>=1.0.0\n  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\nRequirement already satisfied: packaging in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from pytest==7.0.1->gym[all]) (21.3)\nCollecting iniconfig\n  Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\nCollecting py>=1.8.2\n  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pluggy<2.0,>=0.12 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from pytest==7.0.1->gym[all]) (1.0.0)\nRequirement already satisfied: attrs>=19.2.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from pytest==7.0.1->gym[all]) (22.1.0)\nCollecting importlib-resources\n  Downloading importlib_resources-6.0.0-py3-none-any.whl (31 kB)\nRequirement already satisfied: typing-extensions in /shared-libs/python3.9/py/lib/python3.9/site-packages (from ale-py~=0.8.0->gym[all]) (4.4.0)\nRequirement already satisfied: pillow>=8.3.2 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from imageio>=2.14.1->gym[all]) (9.2.0)\nRequirement already satisfied: zipp>=0.5 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gym[all]) (3.9.0)\nRequirement already satisfied: fonttools>=4.22.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib>=3.0->gym[all]) (4.37.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib>=3.0->gym[all]) (1.4.4)\nRequirement already satisfied: python-dateutil>=2.7 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from matplotlib>=3.0->gym[all]) (2.8.2)\nRequirement already satisfied: contourpy>=1.0.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib>=3.0->gym[all]) (1.0.5)\nRequirement already satisfied: cycler>=0.10 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from matplotlib>=3.0->gym[all]) (0.11.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from matplotlib>=3.0->gym[all]) (3.0.9)\nCollecting decorator<5.0,>=4.0.2\n  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\nRequirement already satisfied: tqdm<5.0,>=4.11.2 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from moviepy>=1.0.0->gym[all]) (4.64.1)\nRequirement already satisfied: requests<3.0,>=2.8.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from moviepy>=1.0.0->gym[all]) (2.28.1)\nCollecting proglog<=1.0.0\n  Downloading proglog-0.1.10-py3-none-any.whl (6.1 kB)\nCollecting imageio_ffmpeg>=0.2.0\n  Downloading imageio_ffmpeg-0.4.8-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fasteners~=0.15\n  Downloading fasteners-0.18-py3-none-any.whl (18 kB)\nCollecting Cython>=0.27.2\n  Downloading Cython-3.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: cffi>=1.10 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from mujoco-py<2.2,>=2.1->gym[all]) (1.15.1)\nRequirement already satisfied: pycparser in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1->gym[all]) (2.21)\nRequirement already satisfied: six>=1.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=3.0->gym[all]) (1.16.0)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gym[all]) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gym[all]) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gym[all]) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gym[all]) (2.1.1)\nBuilding wheels for collected packages: box2d-py, moviepy, gym\n  Building wheel for box2d-py (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp39-cp39-linux_x86_64.whl size=2881607 sha256=e756933ea1fa224b30079ba6d757092ff1575eabccecd60355f0c4f0521ec26d\n  Stored in directory: /root/.cache/pip/wheels/a4/c2/c1/076651c394f05fe60990cd85616c2d95bc1619aa113f559d7d\n  Building wheel for moviepy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for moviepy: filename=moviepy-1.0.3-py3-none-any.whl size=110744 sha256=ceb329e59fe532e7608f486acee74416033b60c21ea13765036123c31b43ad27\n  Stored in directory: /root/.cache/pip/wheels/29/15/e4/4f790bec6acd51a00b67e8ee1394f0bc6e0135c315f8ff399a\n  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827647 sha256=b50ff4280072b849229110191190b35201e4f919947ed9598d503c6567520b14\n  Stored in directory: /root/.cache/pip/wheels/af/2b/30/5e78b8b9599f2a2286a582b8da80594f654bf0e18d825a4405\nSuccessfully built box2d-py moviepy gym\nInstalling collected packages: pyopengl, gym-notices, glfw, box2d-py, tomli, pygame, py, proglog, opencv-python, mujoco, lz4, iniconfig, importlib-resources, imageio_ffmpeg, imageio, fasteners, decorator, Cython, cloudpickle, pytest, mujoco-py, moviepy, gym, ale-py\n  Attempting uninstall: decorator\n    Found existing installation: decorator 5.1.1\n    Not uninstalling decorator at /shared-libs/python3.9/py-core/lib/python3.9/site-packages, outside environment /root/venv\n    Can't uninstall 'decorator'. No files were found to uninstall.\nSuccessfully installed Cython-3.0.0 ale-py-0.8.1 box2d-py-2.3.5 cloudpickle-2.2.1 decorator-4.4.2 fasteners-0.18 glfw-2.6.2 gym-0.26.2 gym-notices-0.0.8 imageio-2.31.1 imageio_ffmpeg-0.4.8 importlib-resources-6.0.0 iniconfig-2.0.0 lz4-4.3.2 moviepy-1.0.3 mujoco-2.2.0 mujoco-py-2.1.2.14 opencv-python-4.8.0.74 proglog-0.1.10 py-1.11.0 pygame-2.1.0 pyopengl-3.1.7 pytest-7.0.1 tomli-2.0.1\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: imageio in /root/venv/lib/python3.9/site-packages (2.31.1)\nRequirement already satisfied: numpy in /shared-libs/python3.9/py/lib/python3.9/site-packages (from imageio) (1.23.4)\nRequirement already satisfied: pillow>=8.3.2 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from imageio) (9.2.0)\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m",
          "output_type": "stream"
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": "import math\nimport gym\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport wandb\nimport time\nimport imageio",
      "metadata": {
        "source_hash": "c8996786",
        "execution_start": 1690103530030,
        "execution_millis": 1830,
        "deepnote_app_coordinates": {
          "h": 9,
          "w": 12,
          "x": 0,
          "y": 1199
        },
        "deepnote_to_be_reexecuted": false,
        "cell_id": "e40c85e04019493c9e66dfc45cb22159",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": "## Sweeps configurations",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 5,
          "w": 12,
          "x": 0,
          "y": 1209
        },
        "cell_id": "6578d3912faf4769bf20e768e96e27b7",
        "deepnote_cell_type": "text-cell-h2"
      }
    },
    {
      "cell_type": "markdown",
      "source": "At first I thought of keeping the states with ranges of [10, 20, 30, 40] But then I quickly noticed that I encounter the \"curse of dimensionality\". Which with fast calculation gets really problematic in terms of RAM:",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 72,
            "fromCodePoint": 56
          }
        ],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1215
        },
        "cell_id": "8943ce828ff2436b90600b916e7746da",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "40^8*4 * 4 bytes in GB = 104,857.6 GB\n20^8*4 * 4 bytes in GB = 409.6 GB\n10^8*4 * 4 bytes in GB = 1.6 GB",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "bold": true
            },
            "toCodePoint": 34,
            "fromCodePoint": 25
          },
          {
            "type": "marks",
            "marks": {
              "bold": true
            },
            "toCodePoint": 68,
            "fromCodePoint": 63
          },
          {
            "type": "marks",
            "marks": {
              "bold": true
            },
            "toCodePoint": 100,
            "fromCodePoint": 97
          }
        ],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1218
        },
        "cell_id": "5d3aea2f64e34b94b1d56eb74f3c8f94",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "So to cope with this I had to lower the ranges of the states to something more feasible, we can manipulate this function to something which would be more comfortable to us:",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1221
        },
        "cell_id": "011976b709ff498295d28f5b56f9250d",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "![Ram usage calculation](image-20230705-104029.png)\nwhere the <code>RAM</code> is our target RAM in GB and the <code>4 bytes</code> indicates the approximate size of each cell and the last <code>4</code> indicates the the number of actions.",
      "metadata": {
        "deepnote_app_coordinates": {
          "h": 6,
          "w": 12,
          "x": 0,
          "y": 1227
        },
        "cell_id": "b67598c1317c43f792fcf6d7a7b38b38",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "I have 5GB available for this machine so we can use this calculation to get the approximations for the required dimentions and round it to lower for our comfortability:",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1234
        },
        "cell_id": "3da5662ec0334a41abd0de80b021f9db",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "⌊root(8)(5/((4 bytes * 4) in GB))⌋ = 11 ; ⌊root(8)(1/((4 bytes * 4) in GB))⌋ = 9",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 80,
            "fromCodePoint": 0
          }
        ],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1237
        },
        "cell_id": "3a96cc83ab374b349b4288707b59e770",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "We can see that ideally our upper bound is 11 which is not very helpful, also we know that our machine would need to use some RAM for other parameters and calculations so we would have to suffice with bucket sizes of max 10 and minimum 4.",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 237,
            "fromCodePoint": 217
          }
        ],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1240
        },
        "cell_id": "b0de7ac3a6544bc0a17114bd58fb8f19",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "Also in comparison with the previous notebook I learned that we can use ranges in the sweep configuration like you see below:",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1243
        },
        "cell_id": "2d3e1141fe6c41038ff57a8c3e79523b",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "code",
      "source": "sweep_config = {\n    'method': 'bayes',\n    'metric': {\n        'name': 'avg_reward',\n        'goal': 'maximize'\n    },\n    'parameters': {\n        'learning_rate': {\n            'min': 0.001, \n            'max': 0.1, \n            'distribution': 'uniform'\n        },\n        'discount': {\n            'min': 0.8, \n            'max': 1.0, \n            'distribution': 'uniform'\n        },\n        'epsilon': {\n            'min': 0.1, \n            'max': 1.0, \n            'distribution': 'uniform'\n        },\n        'num_states': {\n            'min': 4,\n            'max': 10,\n            'distribution': 'int_uniform'\n        },\n        'exploration_strategy': {\n            'values': ['epsilon_greedy', 'ucb']\n        },\n        'ucb_c': {\n            'min': 0.1, \n            'max': 2, \n            'distribution': 'uniform'\n        },\n        'init_q_value': {\n            'min': 0, \n            'max': 200, \n            'distribution': 'int_uniform'\n        },\n        'init_q_state': {\n            'values': ['random']\n        },\n        'end_epsilon_decay': {\n            'values': [0.1, 0.25, 0.5, 1, 2]\n        }\n    },\n    'count': 20  # limit sweep to 20 runs\n}",
      "metadata": {
        "source_hash": "363045a4",
        "execution_start": 1690103531833,
        "execution_millis": 33,
        "deepnote_app_coordinates": {
          "h": 35,
          "w": 12,
          "x": 0,
          "y": 1246
        },
        "deepnote_to_be_reexecuted": false,
        "cell_id": "3e8e517a91964ddf8d7764219f244028",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": "Number of episodes to iterate, at first I ran the tests at 25K episodes, but then I noticed that some training were \"cut\" in the middle end (Suggesting that we might benefit from using more episodes and that not all runs converge)",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "strike": true
            },
            "toCodePoint": 135,
            "fromCodePoint": 129
          }
        ],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1282
        },
        "cell_id": "410f521785bf4e41b22a4015b7f1c018",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "code",
      "source": "EPISODES = 50000",
      "metadata": {
        "source_hash": "fedfced7",
        "execution_start": 1690103531838,
        "execution_millis": 16,
        "deepnote_slider_step": 500,
        "deepnote_variable_name": "EPISODES",
        "deepnote_variable_value": "50000",
        "deepnote_app_coordinates": {
          "h": 4,
          "w": 8,
          "x": 0,
          "y": 1285
        },
        "deepnote_slider_max_value": 100000,
        "deepnote_slider_min_value": 1000,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "3d464ef403e4477b85e911c371fec823",
        "deepnote_cell_type": "input-slider"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": "## Helper Functions",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 5,
          "w": 12,
          "x": 0,
          "y": 1290
        },
        "cell_id": "d4dd5846fb1743e9b2da41d3e9e89f46",
        "deepnote_cell_type": "text-cell-h2"
      }
    },
    {
      "cell_type": "markdown",
      "source": "choose_action_greedy - This function selects an action for a given state based on the Q-table values. It implements the ε-greedy exploration strategy, which makes a trade-off between exploration and exploitation. If a random number is greater than ε (epsilon), it chooses the action with the highest Q-value for the given state (exploitation). Otherwise, it selects a random action (exploration).",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 20,
            "fromCodePoint": 0
          }
        ],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1296
        },
        "cell_id": "7cf556bdcc0c4f5a854c08df87b24d03",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "choose_action_UCB - This function also selects an action for a given state, but uses the Upper Confidence Bound (UCB) algorithm. This algorithm balances exploitation and exploration by choosing the action with the highest combined Q-value and exploration function value. The exploration function increases as the action is chosen less frequently, leading to a more balanced exploration of the action space.",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 17,
            "fromCodePoint": 0
          }
        ],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1299
        },
        "cell_id": "8b34b1676d6a40e2bd55e97cc29fc872",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "UCB is based on the principle of optimism in the face of uncertainty, meaning it tends to prefer actions that have not been tried often. The core idea of UCB is to choose the action that has the maximum upper confidence bound, which is a sum of the current estimated value of the action and an uncertainty term. The uncertainty term increases with fewer trials of an action, thereby making less tried actions more attractive.The UCB action selection formula is as follows:",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1302
        },
        "cell_id": "69caaebef2784eebbafa2e3e8411e47f",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "<img src=\"/work/SCR-20230626-rqjk.png\">Where <code>t</code> is the total number of actions taken so far, <code>Nt(a)</code> is the number of times action <code>a</code> has been taken, The number <code>c > 0</code> controls the degree of exploration. Important thing we have to take into account is that under the root we cannot have negative numbers, therefore I had to restrict the values in the <code>q_table</code> to be <code>>= 0</code></img>\n\n\n",
      "metadata": {
        "deepnote_app_coordinates": {
          "h": 6,
          "w": 12,
          "x": 0,
          "y": 1305
        },
        "cell_id": "55d2b1451a264b469b2da99e077ea3cb",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "update_q_table - This function updates the Q-table using the Q-Learning update rule. It calculates the new Q-value for the state-action pair as a weighted average of the old value and the learned value, where the learned value is the sum of the reward and the discounted estimate of the optimal future Q-value.",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 14,
            "fromCodePoint": 0
          }
        ],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1312
        },
        "cell_id": "c2c95440059b459b966f6117f0d29dbc",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "An important mention in this function that was made due to the UCB algorithm was that I had to set the lower bound of the rewards to 0 (Due to the ln() function and the root that might get to there).",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "bold": true
            },
            "toCodePoint": 76,
            "fromCodePoint": 63
          }
        ],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1315
        },
        "cell_id": "81daafd037334a07b922c584ed14677c",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "log_metrics - This function logs various metrics after each episode, including the reward of the episode, the maximum and minimum rewards obtained so far, the average reward over the last 100 episodes, and the exploration strategy used.",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 11,
            "fromCodePoint": 0
          }
        ],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1318
        },
        "cell_id": "8dafa709e32143639eec9b0174a9dc4d",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "get_q_table_shape - Is a helper function for the initiation ( I have seperated it to make the code more readable )",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 17,
            "fromCodePoint": 0
          }
        ],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1321
        },
        "cell_id": "a4693f2d731949d989b52b7275ab2cf2",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "code",
      "source": "def discretize_state(state):\n    digitized_state = []\n    for i in range(len(RANGES)):  # Adjust this if the number of states changes\n        if i < 6:  # This is a continuous variable\n            digitized_state.append(np.digitize(state[i], RANGES[i]))\n        else:  # This is a boolean variable\n            digitized_state.append(int(state[i]))\n    return tuple(digitized_state)\n\n\ndef reset_environment():\n    observation, info = ENV.reset()\n    discrete_state = discretize_state(observation)\n    return observation, discrete_state\n\n\ndef choose_action_greedy(discrete_state):\n    if np.random.random() > EPSILON:\n        action = np.argmax(Q_TABLE[discrete_state])\n    else:\n        action = np.random.randint(0, ENV.action_space.n)\n    return action\n\n\ndef choose_action_UCB(discrete_state):\n    c = UCB_C\n    ucb_values = Q_TABLE[discrete_state] + c * np.sqrt(\n        np.log(np.sum(Q_TABLE[discrete_state]) + 1) / (Q_TABLE[discrete_state] + 1e-5))\n    action = np.argmax(ucb_values)\n    return action\n\n\ndef update_q_table(discrete_state, action, reward, new_discrete_state):\n    global Q_TABLE\n    max_future_q = np.max(Q_TABLE[new_discrete_state])  # estimate of optimal future value\n    current_q = Q_TABLE[discrete_state + (action,)]  # current Q-value\n    new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n    if CONFIG.exploration_strategy == 'ucb': # apply lower bound only if UCB strategy\n        new_q =  0 if new_q < 0 else new_q # set lower bound 0 to avoid negative numbers to accomodate UCB algorithm\n    Q_TABLE[discrete_state + (action,)] = new_q # update Q-table with new Q-value\n\n\ndef log_metrics(run, reward_list, max_reward_list, min_reward_list, episode_reward, episode_steps, duration):\n    reward_list.append(episode_reward)\n    max_reward_list.append(max(episode_reward, max_reward_list[-1]) if max_reward_list else episode_reward)\n    min_reward_list.append(min(episode_reward, min_reward_list[-1]) if min_reward_list else episode_reward)\n    avg_reward = np.mean(reward_list[-100:])  # average over last 100 episodes\n    metrics = {'eps': 1 / duration, 'reward': episode_reward, 'steps': episode_steps,\n               'avg_reward': avg_reward, 'max_reward': max_reward_list[-1],\n               'min_reward': min_reward_list[-1]}\n    run.log(metrics)\n\n\ndef get_q_table_shape():\n    shapes = []\n    for i in range(8):  # Adjust this if the number of states changes\n        if i < 6:  # This is a continuous variable\n            shapes.append(len(RANGES[i]) + 1)\n        else:  # This is a boolean variable\n            shapes.append(2)  # This accounts for the two states (0 and 1)\n    shapes.append(ENV.action_space.n)  # Add size of action space at the end\n    return tuple(shapes)\n",
      "metadata": {
        "source_hash": "7b91a2ad",
        "execution_start": 1690103531854,
        "execution_millis": 34,
        "deepnote_app_coordinates": {
          "h": 46,
          "w": 12,
          "x": 0,
          "y": 1324
        },
        "deepnote_to_be_reexecuted": false,
        "cell_id": "68027a95adb24caaa6e4055a3b87a446",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": "## Running the episodes",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 5,
          "w": 12,
          "x": 0,
          "y": 1371
        },
        "cell_id": "3c30a156b50443c0a583e3f46ecf7fab",
        "deepnote_cell_type": "text-cell-h2"
      }
    },
    {
      "cell_type": "markdown",
      "source": "Like the title and the function name states, this part is responsible on running the actual episodes (The training process).",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1377
        },
        "cell_id": "62a53587f3044c8baeb3d7ec3ceead7a",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "We start by initiating some logging helping arrays that would hold the information on the max/min rewards. From there we continue to the main for loop that would be running all the episodes, I have used here the method tqdm which helps me visualise the approximate time left for the run and the progress so far - You can find the documentation here.",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 105,
            "fromCodePoint": 90
          },
          {
            "url": "https://tqdm.github.io",
            "type": "link",
            "ranges": [],
            "toCodePoint": 348,
            "fromCodePoint": 344
          }
        ],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1380
        },
        "cell_id": "04cfececd4b643569130a503364b9976",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "In this section I firstly initiated and empty array for the frames that would capture the last 3 runs (To later create a gif from them to nicely visualise the results). ",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1383
        },
        "cell_id": "b431475e17ae41d899ba74bdf2c45df8",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "After that we initiate the start time for this episode (For additional metrics) and also the episode_reward and the number of steps made for this episode.",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1386
        },
        "cell_id": "f639189618b14d0e93f724b7f4e259bb",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### The calculations",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 5,
          "w": 12,
          "x": 0,
          "y": 1389
        },
        "cell_id": "f6584844c36447c497f3cb3fb48edffd",
        "deepnote_cell_type": "text-cell-h3"
      }
    },
    {
      "cell_type": "markdown",
      "source": "We initially reset the environment to it's initial state observation, discrete_state = reset_environment() and we enter the main while loop which would only terminate if we were terminated or truncated (Either the spaceship was flying too long/ crashed/ landed successfully). This is important as we don't want to run indeffinetly or run too short without reaching the end goal.",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 106,
            "fromCodePoint": 57
          },
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 188,
            "fromCodePoint": 178
          },
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 201,
            "fromCodePoint": 192
          }
        ],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1395
        },
        "cell_id": "441601c4803447b5a965cfe7d04349d0",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "From there we run the relevant selection algorithm which selects based on the strategy provided by the W&B configuration.\n```python\nif STRATEGY == 'ucb':\n    action = choose_action_UCB(discrete_state)\nelse:\n    action = choose_action_greedy(discrete_state)\n```",
      "metadata": {
        "deepnote_app_coordinates": {
          "h": 9,
          "w": 12,
          "x": 0,
          "y": 1398
        },
        "cell_id": "b4d080c5e0994f9785d9ba97cbd2b6f0",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "From there we perform a step and receive all the nessecary infomration from it observation, reward, terminated, truncated, info. And right after it we descritisize the returned observation to lower the dimentions - This is another subject which would be explored in the next upcoming notes.",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 127,
            "fromCodePoint": 79
          }
        ],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1408
        },
        "cell_id": "246e73df3dca4794a47fbccc955db194",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "Finally if everything went successfully we get to update the Q table with the new values using the following formula:<image src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/d247db9eaad4bd343e7882ec546bf3847ebd36d8\"></image><ul>\n<li>Q(s,a) is the current estimate of the Q-value for the state-action pair (s, a)</li><li>α is the learning rate</li><li>r is the immediate reward obtained after taking action a in state s</li><li>γ is the discount factor</li><li>max Q(s',a') is the maximum Q-value over all actions a' in the next state s'</li></ul>",
      "metadata": {
        "deepnote_app_coordinates": {
          "h": 10,
          "w": 12,
          "x": 0,
          "y": 1411
        },
        "cell_id": "bf2acad379a64033aa41a4afbf7a0ded",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "Another important part of the code is the following section:\n```python\nif (END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING) and STRATEGY == 'epsilon_greedy':\n    EPSILON -= EPSILON_DECAY_VALUE\n```\nWithout this section we cannot perform the fnctionality of epsilon decaying which should lower the epsilon value after some episodes (Resulting in more stable optimization by the end and less exploration).\n\nFinally we perfomr some additional gif saving functionality for nice visualisations.",
      "metadata": {
        "deepnote_app_coordinates": {
          "h": 10,
          "w": 12,
          "x": 0,
          "y": 1422
        },
        "cell_id": "bd52947df9bc468599359e848f90654f",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "def run_episodes(run):\n    global EPSILON\n    # Additional data lists\n    reward_list = []\n    max_reward_list = []\n    min_reward_list = []\n    print(\"Starting to run episodes\")\n\n    for episode in tqdm(range(EPISODES), desc=\"Training\", unit=\"episode\"):\n        frames = [] # for saving the simulations\n        is_saving_video = (episode + 3 >= EPISODES) # How many runs to log\n        start_time = time.time()\n        episode_reward = 0  # initialize the reward for this episode\n        episode_steps = 0  # initialize the number of steps for this episode\n\n        observation, discrete_state = reset_environment()\n\n        terminated = False\n        truncated = False\n        reward = 0\n        done = False\n        while not terminated and not truncated and not done:\n            if is_saving_video:\n                frame = ENV.render()\n                frames.append(frame)\n\n            if STRATEGY == 'ucb':\n                action = choose_action_UCB(discrete_state)\n            else:\n                action = choose_action_greedy(discrete_state)\n\n            observation, reward, terminated, truncated, done = ENV.step(action)\n            new_discrete_state = discretize_state(observation)\n\n            if not terminated and not truncated:\n                update_q_table(discrete_state, action, reward, new_discrete_state)\n            else:\n                update_q_table(discrete_state, action, reward, 0) # max future q is 0 for terminal state \n                \n\n            discrete_state = new_discrete_state\n            episode_reward += reward\n            episode_steps += 1\n\n        end_time = time.time()\n        duration = end_time - start_time\n\n        log_metrics(run, reward_list, max_reward_list, min_reward_list, episode_reward, episode_steps, duration)\n\n        if (END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING) and STRATEGY == 'epsilon_greedy':\n            EPSILON -= EPSILON_DECAY_VALUE\n        \n        if is_saving_video:\n            # Save the frames as a GIF file\n            imageio.mimsave(f'run_{episode}.gif', frames, format='GIF', duration=(1000 * 1/30))\n            frames.clear()  # Clear the frame list\n            run.log({\"simulations\": wandb.Video(f'run_{episode}.gif',format=\"gif\")})",
      "metadata": {
        "source_hash": "63ee6840",
        "execution_start": 1690103531870,
        "execution_millis": 11,
        "deepnote_app_coordinates": {
          "h": 42,
          "w": 12,
          "x": 0,
          "y": 1433
        },
        "deepnote_to_be_reexecuted": false,
        "cell_id": "374e7b1610dc4de4ae07da206835b7a1",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": "## Training function",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 5,
          "w": 12,
          "x": 0,
          "y": 1476
        },
        "cell_id": "c807355e5a1346d08afe2f5506089d4a",
        "deepnote_cell_type": "text-cell-h2"
      }
    },
    {
      "cell_type": "markdown",
      "source": "In this function we perform all of the \"janitorial functionality\" which is setting up global variables (For some reason jupiter notes don't work well without it and passing the values in the function is too messy) and unpack the configurations we received from wandb which is the Weights and Biases library that grants us access to the configurations for the current sweep.",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "italic": true
            },
            "toCodePoint": 64,
            "fromCodePoint": 40
          },
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 266,
            "fromCodePoint": 261
          },
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 372,
            "fromCodePoint": 367
          }
        ],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1482
        },
        "cell_id": "aa9e7c62653b40babf59f8594d05a6a2",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "code",
      "source": "def train():\n    global LEARNING_RATE\n    global DISCOUNT\n    global EPSILON\n    global START_EPSILON_DECAYING\n    global END_EPSILON_DECAYING\n    global EPSILON_DECAY_VALUE\n    global UCB_C\n    global STRATEGY\n    global RANGES\n    global Q_TABLE\n    global CONFIG\n    global EPSILON\n    global ENV\n\n    # Initialize a new wandb run\n    run = wandb.init(config=wandb.config)\n\n    # Config is a variable that holds and saves hyperparameters and inputs\n    CONFIG = wandb.config\n\n    LEARNING_RATE = CONFIG.learning_rate\n    DISCOUNT = CONFIG.discount\n    EPSILON = CONFIG.epsilon if CONFIG.exploration_strategy == 'epsilon_greedy' else None\n    START_EPSILON_DECAYING = 1\n    END_EPSILON_DECAYING = EPISODES // CONFIG.end_epsilon_decay\n    EPSILON_DECAY_VALUE = EPSILON / (\n                END_EPSILON_DECAYING - START_EPSILON_DECAYING) if CONFIG.exploration_strategy == 'epsilon_greedy' else None\n    UCB_C = CONFIG.ucb_c if CONFIG.exploration_strategy == 'ucb' else None\n    STRATEGY = CONFIG.exploration_strategy\n\n    ENV = gym.make('LunarLander-v2', render_mode='rgb_array')\n\n    num_states_continuous = CONFIG.num_states  # The number of bins for continuous variables\n    num_states_boolean = 2  # The number of bins for boolean variables\n\n    # Define the range of each dimension\n    RANGES = [\n        np.linspace(-90, 90, num_states_continuous),  # X coordinate\n        np.linspace(-90, 90, num_states_continuous),  # Y coordinate\n        np.linspace(-5, 5, num_states_continuous),  # X velocity\n        np.linspace(-5, 5, num_states_continuous),  # Y velocity\n        np.linspace(-np.pi, np.pi, num_states_continuous),  # Angle\n        np.linspace(-5, 5, num_states_continuous),  # Angular velocity\n        np.linspace(0, 1, num_states_boolean),  # Left leg contact\n        np.linspace(0, 1, num_states_boolean)  # Right leg contact\n    ]\n    print(RANGES)\n    print(CONFIG)\n    # Initialize Q-table with the defined initial Q value\n    \n    if CONFIG.init_q_state == 'static':\n        Q_TABLE = np.full(shape=get_q_table_shape(), fill_value=CONFIG.init_q_value)\n    else:\n        Q_TABLE = np.random.uniform(low=0, high=CONFIG.init_q_value, size=get_q_table_shape())\n\n    run_episodes(run)\n\n    # Save the Q-table as an Artifact\n    artifact = wandb.Artifact('q_table', type='model')\n    np.save('q_table.npy', Q_TABLE)\n    artifact.add_file('q_table.npy')\n    run.log_artifact(artifact)\n\n    ENV.close()\n    run.finish()  # End the Run",
      "metadata": {
        "source_hash": "18803f67",
        "execution_start": 1690103531881,
        "execution_millis": 23,
        "deepnote_app_coordinates": {
          "h": 45,
          "w": 12,
          "x": 0,
          "y": 1485
        },
        "deepnote_to_be_reexecuted": false,
        "cell_id": "872f06bb2e184fcda8582bdde24fe5fb",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": "## Sweep initialization",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 5,
          "w": 12,
          "x": 0,
          "y": 1531
        },
        "cell_id": "277e2ec73f704fa080324cbf2d0d6256",
        "deepnote_cell_type": "text-cell-h2"
      }
    },
    {
      "cell_type": "markdown",
      "source": "This part is simply running the sweep with the given configurations in section 2.i.",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "bold": true
            },
            "toCodePoint": 83,
            "fromCodePoint": 79
          }
        ],
        "deepnote_app_coordinates": {
          "h": 2,
          "w": 8,
          "x": 0,
          "y": 1537
        },
        "cell_id": "3d7eb3b7b9734ad0802dcb6da2e639c9",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "code",
      "source": "sweep_id = wandb.sweep(sweep_config, project=\"lunar-lander-v2\")\nwandb.agent(sweep_id, train)",
      "metadata": {
        "source_hash": "b3d59b6c",
        "execution_start": 1690103531901,
        "execution_millis": 18060513,
        "deepnote_app_coordinates": {
          "h": 798,
          "w": 12,
          "x": 0,
          "y": 1540
        },
        "deepnote_to_be_reexecuted": false,
        "cell_id": "e7df1ab35a4b4cdcad580e6930328c8a",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m To avoid this, please fix the sweep config schema violations below:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 1. Additional properties are not allowed ('count' was unexpected)\nCreate sweep with ID: 1itf7z14\nSweep URL: https://wandb.ai/got-tree/lunar-lander-v2/sweeps/1itf7z14\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 08m5q6nf with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdiscount: 0.8980840047672926\n\u001b[34m\u001b[1mwandb\u001b[0m: \tend_epsilon_decay: 0.25\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon: 0.26847381425975003\n\u001b[34m\u001b[1mwandb\u001b[0m: \texploration_strategy: ucb\n\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_q_state: random\n\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_q_value: 93\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.08778374101568641\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_states: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tucb_c: 1.9155583414446835\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtcivie\u001b[0m (\u001b[33mgot-tree\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
          "output_type": "stream"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.15.5"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/work/wandb/run-20230723_091418-08m5q6nf</code>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/got-tree/lunar-lander-v2/runs/08m5q6nf' target=\"_blank\">young-sweep-1</a></strong> to <a href='https://wandb.ai/got-tree/lunar-lander-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/got-tree/lunar-lander-v2/sweeps/1itf7z14' target=\"_blank\">https://wandb.ai/got-tree/lunar-lander-v2/sweeps/1itf7z14</a>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/got-tree/lunar-lander-v2' target=\"_blank\">https://wandb.ai/got-tree/lunar-lander-v2</a>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View sweep at <a href='https://wandb.ai/got-tree/lunar-lander-v2/sweeps/1itf7z14' target=\"_blank\">https://wandb.ai/got-tree/lunar-lander-v2/sweeps/1itf7z14</a>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/got-tree/lunar-lander-v2/runs/08m5q6nf' target=\"_blank\">https://wandb.ai/got-tree/lunar-lander-v2/runs/08m5q6nf</a>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "text": "[array([-90., -70., -50., -30., -10.,  10.,  30.,  50.,  70.,  90.]), array([-90., -70., -50., -30., -10.,  10.,  30.,  50.,  70.,  90.]), array([-5.        , -3.88888889, -2.77777778, -1.66666667, -0.55555556,\n        0.55555556,  1.66666667,  2.77777778,  3.88888889,  5.        ]), array([-5.        , -3.88888889, -2.77777778, -1.66666667, -0.55555556,\n        0.55555556,  1.66666667,  2.77777778,  3.88888889,  5.        ]), array([-3.14159265, -2.44346095, -1.74532925, -1.04719755, -0.34906585,\n        0.34906585,  1.04719755,  1.74532925,  2.44346095,  3.14159265]), array([-5.        , -3.88888889, -2.77777778, -1.66666667, -0.55555556,\n        0.55555556,  1.66666667,  2.77777778,  3.88888889,  5.        ]), array([0., 1.]), array([0., 1.])]\n{'discount': 0.8980840047672926, 'end_epsilon_decay': 0.25, 'epsilon': 0.26847381425975003, 'exploration_strategy': 'ucb', 'init_q_state': 'random', 'init_q_value': 93, 'learning_rate': 0.08778374101568641, 'num_states': 10, 'ucb_c': 1.9155583414446835}\nStarting to run episodes\nTraining: 100%|██████████| 50000/50000 [2:10:59<00:00,  6.36episode/s]\n",
          "output_type": "stream"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>█▂▇▂▃▂▃▂▂▂▁▇▆▄▆▆▆▅▆▆▆▂▂▁▁▂▂▂▂▁▂▂▂▁▁▂▂▁▁▁</td></tr><tr><td>eps</td><td>▃▃▂▂▄▇▂▆▃▃▃▂▅█▇▅▂▃▇▅▃▂▂▂▄▁▂▃▂▅▂▁▁▃▁▂▂▂▂▄</td></tr><tr><td>max_reward</td><td>▁▁██████████████████████████████████████</td></tr><tr><td>min_reward</td><td>█▅▅▅▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>reward</td><td>█▇█▅▇█▄▅▅▅▆█▅▆▅██▇▅█▆▇▅▁▆▁▅▇▃▆▄▅▄▅▃▆▅▅▅▅</td></tr><tr><td>steps</td><td>▃▂▃▄▁▂▃▂▂▂▂▃▂▁▂▃▃▂▂▁▂▂▃▆▁█▂▂▅▁▃▃▃▃▅▂▂▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-734.87708</td></tr><tr><td>eps</td><td>0.94618</td></tr><tr><td>max_reward</td><td>269.84585</td></tr><tr><td>min_reward</td><td>-3357.77714</td></tr><tr><td>reward</td><td>-627.10189</td></tr><tr><td>steps</td><td>80</td></tr></table><br/></div></div>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">young-sweep-1</strong> at: <a href='https://wandb.ai/got-tree/lunar-lander-v2/runs/08m5q6nf' target=\"_blank\">https://wandb.ai/got-tree/lunar-lander-v2/runs/08m5q6nf</a><br/>Synced 5 W&B file(s), 3 media file(s), 1 artifact file(s) and 0 other file(s)"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20230723_091418-08m5q6nf/logs</code>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: olp90i7h with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdiscount: 0.9735989520808894\n\u001b[34m\u001b[1mwandb\u001b[0m: \tend_epsilon_decay: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon: 0.3076643475463193\n\u001b[34m\u001b[1mwandb\u001b[0m: \texploration_strategy: epsilon_greedy\n\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_q_state: random\n\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_q_value: 194\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.08271768521347939\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_states: 6\n\u001b[34m\u001b[1mwandb\u001b[0m: \tucb_c: 0.1860104764541042\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
          "output_type": "stream"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.15.5"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/work/wandb/run-20230723_112545-olp90i7h</code>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/got-tree/lunar-lander-v2/runs/olp90i7h' target=\"_blank\">glowing-sweep-2</a></strong> to <a href='https://wandb.ai/got-tree/lunar-lander-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/got-tree/lunar-lander-v2/sweeps/1itf7z14' target=\"_blank\">https://wandb.ai/got-tree/lunar-lander-v2/sweeps/1itf7z14</a>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/got-tree/lunar-lander-v2' target=\"_blank\">https://wandb.ai/got-tree/lunar-lander-v2</a>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View sweep at <a href='https://wandb.ai/got-tree/lunar-lander-v2/sweeps/1itf7z14' target=\"_blank\">https://wandb.ai/got-tree/lunar-lander-v2/sweeps/1itf7z14</a>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/got-tree/lunar-lander-v2/runs/olp90i7h' target=\"_blank\">https://wandb.ai/got-tree/lunar-lander-v2/runs/olp90i7h</a>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "text": "[array([-90., -54., -18.,  18.,  54.,  90.]), array([-90., -54., -18.,  18.,  54.,  90.]), array([-5., -3., -1.,  1.,  3.,  5.]), array([-5., -3., -1.,  1.,  3.,  5.]), array([-3.14159265, -1.88495559, -0.62831853,  0.62831853,  1.88495559,\n        3.14159265]), array([-5., -3., -1.,  1.,  3.,  5.]), array([0., 1.]), array([0., 1.])]\n{'discount': 0.9735989520808894, 'end_epsilon_decay': 2, 'epsilon': 0.3076643475463193, 'exploration_strategy': 'epsilon_greedy', 'init_q_state': 'random', 'init_q_value': 194, 'learning_rate': 0.08271768521347939, 'num_states': 6, 'ucb_c': 0.1860104764541042}\nStarting to run episodes\nTraining: 100%|██████████| 50000/50000 [1:07:34<00:00, 12.33episode/s]\n",
          "output_type": "stream"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▄▁▁▁▂▂▃▁▂▃▃▆▄▂▅▅▅▇█▇▇▇████▇▇██▇▇████▇▇▇█</td></tr><tr><td>eps</td><td>▂▂▂▂▂▅▁▃▂▂▂▁▄▂▃▂▂▂▃▂▂▅▃▅▃▃▆▃▄▄▄▃▅█▄▄▃▄▃▅</td></tr><tr><td>max_reward</td><td>▁▁▁▁▁▁▁▁▆███████████████████████████████</td></tr><tr><td>min_reward</td><td>█▆▆▆▄▄▄▄▄▄▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>reward</td><td>▁▅▅▂▃▄█▇▅▄▁▆▆▃▃▅▂▄▅▅▆▄▅▅█▅▅▅▅▄▄▅▆▅▄▅▅▅▄▅</td></tr><tr><td>steps</td><td>▃▅▅▆▅▁█▄▃▁█▄▁▅▂▃▅▃▄▂▆▁▃▃▅▂▃▁▂▂▃▃▁▂▁▃▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-118.51238</td></tr><tr><td>eps</td><td>0.54105</td></tr><tr><td>max_reward</td><td>302.63775</td></tr><tr><td>min_reward</td><td>-950.45124</td></tr><tr><td>reward</td><td>20.7183</td></tr><tr><td>steps</td><td>93</td></tr></table><br/></div></div>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">glowing-sweep-2</strong> at: <a href='https://wandb.ai/got-tree/lunar-lander-v2/runs/olp90i7h' target=\"_blank\">https://wandb.ai/got-tree/lunar-lander-v2/runs/olp90i7h</a><br/>Synced 5 W&B file(s), 3 media file(s), 1 artifact file(s) and 0 other file(s)"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20230723_112545-olp90i7h/logs</code>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ewfj65rx with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdiscount: 0.896239956837221\n\u001b[34m\u001b[1mwandb\u001b[0m: \tend_epsilon_decay: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon: 0.4704609407197182\n\u001b[34m\u001b[1mwandb\u001b[0m: \texploration_strategy: ucb\n\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_q_state: random\n\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_q_value: 19\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.09511197386588024\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_states: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tucb_c: 0.971857377395093\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
          "output_type": "stream"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.15.5"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/work/wandb/run-20230723_123336-ewfj65rx</code>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/got-tree/lunar-lander-v2/runs/ewfj65rx' target=\"_blank\">different-sweep-3</a></strong> to <a href='https://wandb.ai/got-tree/lunar-lander-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/got-tree/lunar-lander-v2/sweeps/1itf7z14' target=\"_blank\">https://wandb.ai/got-tree/lunar-lander-v2/sweeps/1itf7z14</a>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/got-tree/lunar-lander-v2' target=\"_blank\">https://wandb.ai/got-tree/lunar-lander-v2</a>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View sweep at <a href='https://wandb.ai/got-tree/lunar-lander-v2/sweeps/1itf7z14' target=\"_blank\">https://wandb.ai/got-tree/lunar-lander-v2/sweeps/1itf7z14</a>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/got-tree/lunar-lander-v2/runs/ewfj65rx' target=\"_blank\">https://wandb.ai/got-tree/lunar-lander-v2/runs/ewfj65rx</a>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "text": "[array([-90., -45.,   0.,  45.,  90.]), array([-90., -45.,   0.,  45.,  90.]), array([-5. , -2.5,  0. ,  2.5,  5. ]), array([-5. , -2.5,  0. ,  2.5,  5. ]), array([-3.14159265, -1.57079633,  0.        ,  1.57079633,  3.14159265]), array([-5. , -2.5,  0. ,  2.5,  5. ]), array([0., 1.]), array([0., 1.])]\n{'discount': 0.896239956837221, 'end_epsilon_decay': 2, 'epsilon': 0.4704609407197182, 'exploration_strategy': 'ucb', 'init_q_state': 'random', 'init_q_value': 19, 'learning_rate': 0.09511197386588024, 'num_states': 5, 'ucb_c': 0.971857377395093}\nStarting to run episodes\nTraining: 100%|██████████| 50000/50000 [1:11:19<00:00, 11.68episode/s]\n",
          "output_type": "stream"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>▇▇█▆█▃█▇▇▆▇▅▂▁▅▄▇▃▇█▆▆▅▆▅▆▅▆▇▇▇▆▆▇▆▇▆▅▅▆</td></tr><tr><td>eps</td><td>▂▂▂██▃▂▂▁▂▂▂▃▂▁▂▁▂▄▂▁▁▂▃▁▃▃▃▂▁▂▁▇▃▂▃▁▃▂▁</td></tr><tr><td>max_reward</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>min_reward</td><td>██▅▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>reward</td><td>▇▆▅▇█▆▆▆▅▅▆▁▆▆▅▇▇▄▄▃▂▆▅▆▅▆▄▇▆▅▆▅▅▇▅▆▆▆▆▇</td></tr><tr><td>steps</td><td>▁▂▂▁▁▂▁▁▂▂▂█▁▂▂▁▂▂▂▃▇▂▂▃▃▂▂▁▂▃▃▂▂▁▃▂▄▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_reward</td><td>-582.83334</td></tr><tr><td>eps</td><td>3.40944</td></tr><tr><td>max_reward</td><td>-6.2026</td></tr><tr><td>min_reward</td><td>-1687.01615</td></tr><tr><td>reward</td><td>-445.50621</td></tr><tr><td>steps</td><td>51</td></tr></table><br/></div></div>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">different-sweep-3</strong> at: <a href='https://wandb.ai/got-tree/lunar-lander-v2/runs/ewfj65rx' target=\"_blank\">https://wandb.ai/got-tree/lunar-lander-v2/runs/ewfj65rx</a><br/>Synced 5 W&B file(s), 3 media file(s), 1 artifact file(s) and 0 other file(s)"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20230723_123336-ewfj65rx/logs</code>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x0jnsxyd with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdiscount: 0.8653799683198891\n\u001b[34m\u001b[1mwandb\u001b[0m: \tend_epsilon_decay: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon: 0.1375435431723935\n\u001b[34m\u001b[1mwandb\u001b[0m: \texploration_strategy: epsilon_greedy\n\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_q_state: random\n\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_q_value: 24\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.03992200319270856\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_states: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \tucb_c: 1.0755231557921436\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
          "output_type": "stream"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.15.5"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/work/wandb/run-20230723_134514-x0jnsxyd</code>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/got-tree/lunar-lander-v2/runs/x0jnsxyd' target=\"_blank\">flowing-sweep-4</a></strong> to <a href='https://wandb.ai/got-tree/lunar-lander-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/got-tree/lunar-lander-v2/sweeps/1itf7z14' target=\"_blank\">https://wandb.ai/got-tree/lunar-lander-v2/sweeps/1itf7z14</a>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/got-tree/lunar-lander-v2' target=\"_blank\">https://wandb.ai/got-tree/lunar-lander-v2</a>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View sweep at <a href='https://wandb.ai/got-tree/lunar-lander-v2/sweeps/1itf7z14' target=\"_blank\">https://wandb.ai/got-tree/lunar-lander-v2/sweeps/1itf7z14</a>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/got-tree/lunar-lander-v2/runs/x0jnsxyd' target=\"_blank\">https://wandb.ai/got-tree/lunar-lander-v2/runs/x0jnsxyd</a>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "text": "[array([-90.        , -64.28571429, -38.57142857, -12.85714286,\n        12.85714286,  38.57142857,  64.28571429,  90.        ]), array([-90.        , -64.28571429, -38.57142857, -12.85714286,\n        12.85714286,  38.57142857,  64.28571429,  90.        ]), array([-5.        , -3.57142857, -2.14285714, -0.71428571,  0.71428571,\n        2.14285714,  3.57142857,  5.        ]), array([-5.        , -3.57142857, -2.14285714, -0.71428571,  0.71428571,\n        2.14285714,  3.57142857,  5.        ]), array([-3.14159265, -2.24399475, -1.34639685, -0.44879895,  0.44879895,\n        1.34639685,  2.24399475,  3.14159265]), array([-5.        , -3.57142857, -2.14285714, -0.71428571,  0.71428571,\n        2.14285714,  3.57142857,  5.        ]), array([0., 1.]), array([0., 1.])]\n{'discount': 0.8653799683198891, 'end_epsilon_decay': 1, 'epsilon': 0.1375435431723935, 'exploration_strategy': 'epsilon_greedy', 'init_q_state': 'random', 'init_q_value': 24, 'learning_rate': 0.03992200319270856, 'num_states': 8, 'ucb_c': 1.0755231557921436}\nStarting to run episodes\nTraining:  27%|██▋       | 13728/50000 [27:54<1:36:51,  6.24episode/s]\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\nTraining:  27%|██▋       | 13736/50000 [27:55<1:09:42,  8.67episode/s]",
          "output_type": "stream"
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "source": "# Final results",
      "metadata": {
        "formattedRanges": [],
        "deepnote_app_coordinates": {
          "h": 5,
          "w": 12,
          "x": 0,
          "y": 0
        },
        "cell_id": "a69a2476440641eea0493ca5449e68b6",
        "deepnote_cell_type": "text-cell-h1"
      }
    },
    {
      "cell_type": "markdown",
      "source": "The final results for this run can be seen in this W&B interactive report. I have provided a quick overview on the results from this experiment. I hope you like it and would learn something new from it 🙂",
      "metadata": {
        "formattedRanges": [
          {
            "url": "https://api.wandb.ai/links/got-tree/0tw2h06o",
            "type": "link",
            "ranges": [],
            "toCodePoint": 73,
            "fromCodePoint": 51
          }
        ],
        "deepnote_app_coordinates": {
          "h": 5,
          "w": 12,
          "x": 0,
          "y": 0
        },
        "cell_id": "0014c404864e49d8bde278c391030198",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=5878bf73-13c4-4232-bd61-633eeedc1f05' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote": {},
    "orig_nbformat": 2,
    "deepnote_app_layout": "article",
    "deepnote_notebook_id": "bf000feb697b45f7a8b8a52651683634",
    "deepnote_execution_queue": []
  }
}