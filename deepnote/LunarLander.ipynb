{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Q-Learning on LunarLander-v2 Environment",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "369aba67b2694422a1a7b4e81dac984b",
        "deepnote_cell_type": "text-cell-h1"
      }
    },
    {
      "cell_type": "markdown",
      "source": "In this project, we are training a Q-Learning agent to solve the `LunarLander-v2` environment from OpenAI's Gym.",
      "metadata": {
        "formattedRanges": [
          {
            "url": "https://gymnasium.farama.org/environments/box2d/lunar_lander/",
            "type": "link",
            "ranges": [],
            "toCodePoint": 112,
            "fromCodePoint": 65
          }
        ],
        "cell_id": "14b03a7c5ab44a92a3f58833f918ac2f",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "The objective of the LunarLander-v2 environment is to optimize the trajectory of a spacecraft landing on the moon. The environment is modeled after the classic rocket trajectory optimization problem, with the actions being discrete in nature - either fire the engine at full throttle or keep it off.",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "f66a5eb9d27c40459b3c237bd2fc3b6c",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Training Process",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "578d426f116c4b5297937dc26a29b8a1",
        "deepnote_cell_type": "text-cell-h2"
      }
    },
    {
      "cell_type": "markdown",
      "source": "For the LunarLander problem, I adopted a similar approach as with the MountainCar environment and implemented a Q-Learning algorithm. However, this time I expanded upon the exploration strategy by introducing an Upper Confidence Bound (UCB) policy. My objective was to compare the performance and characteristics of the UCB policy against the ε-greedy policy, a commonly used method in Q-Learning. In addition, I aimed to evaluate the effects of applying an optimistic initialization strategy in combination with the ε-greedy policy.",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "7854a9bb10fd41d782e6fd09b1a122b4",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "The UCB policy is designed to balance exploration and exploitation in reinforcement learning. It leverages uncertainty and variance in the reward estimates to guide the exploration process. This differs from the ε-greedy policy, which explores the action space purely randomly with a certain probability ε.",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "c8b16c6ddd074f3280e6f8fdfd864241",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "On the other hand, optimistic initialization is a simple yet powerful method to encourage exploration in the early stages of training. By initializing the Q-values optimistically (i.e., with higher than achievable values), the agent is incentivized to explore all actions to learn their actual rewards.",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "8c3dd69a53e14666b432800ee43003e2",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Hyper Parameter Tuning",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "bcff3cb65789428697300e80d3889c02",
        "deepnote_cell_type": "text-cell-h2"
      }
    },
    {
      "cell_type": "markdown",
      "source": "We use Weights & Biases Sweeps for hyperparameter tuning. We explore different values of the learning rate, discount factor, and the number of discretized states. The agent's performance is measured by the average reward over 100 episodes.",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "fd63f5da5eac4fc6bd563ff2b97f946d",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "# Code and Running",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "898255adeb8d4c2db29e21d09f6b491c",
        "deepnote_cell_type": "text-cell-h1"
      }
    },
    {
      "cell_type": "markdown",
      "source": "Imports and Installs:",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "72d2e4f208544f4685e509d41610c094",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "code",
      "source": "!pip install wandb\n!pip install gym\n!pip install pygame\n!pip install box2d-py",
      "metadata": {
        "source_hash": "9ed82da",
        "execution_start": 1687804350733,
        "execution_millis": 20261,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "3f8bce9137504debb68552aca3b3a964",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: wandb in /root/venv/lib/python3.9/site-packages (0.15.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from wandb) (3.19.6)\nRequirement already satisfied: requests<3,>=2.0.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from wandb) (2.28.1)\nRequirement already satisfied: typing-extensions in /shared-libs/python3.9/py/lib/python3.9/site-packages (from wandb) (4.4.0)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from wandb) (1.24.0)\nRequirement already satisfied: setproctitle in /root/venv/lib/python3.9/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from wandb) (8.1.3)\nRequirement already satisfied: appdirs>=1.4.3 in /root/venv/lib/python3.9/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /root/venv/lib/python3.9/site-packages (from wandb) (3.1.31)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /root/venv/lib/python3.9/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: setuptools in /root/venv/lib/python3.9/site-packages (from wandb) (58.1.0)\nRequirement already satisfied: psutil>=5.0.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: PyYAML in /root/venv/lib/python3.9/site-packages (from wandb) (6.0)\nRequirement already satisfied: pathtools in /root/venv/lib/python3.9/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: six>=1.4.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /root/venv/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: idna<4,>=2.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: charset-normalizer<3,>=2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\nRequirement already satisfied: smmap<6,>=3.0.1 in /root/venv/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: gym in /root/venv/lib/python3.9/site-packages (0.26.2)\nRequirement already satisfied: gym-notices>=0.0.4 in /root/venv/lib/python3.9/site-packages (from gym) (0.0.8)\nRequirement already satisfied: numpy>=1.18.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from gym) (1.23.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /root/venv/lib/python3.9/site-packages (from gym) (2.2.1)\nRequirement already satisfied: importlib-metadata>=4.8.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from gym) (5.0.0)\nRequirement already satisfied: zipp>=0.5 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gym) (3.9.0)\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: pygame in /root/venv/lib/python3.9/site-packages (2.5.0)\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: box2d-py in /root/venv/lib/python3.9/site-packages (2.3.8)\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m",
          "output_type": "stream"
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": "import math\nimport gym\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport wandb",
      "metadata": {
        "source_hash": "68b14fc",
        "execution_start": 1687804371065,
        "execution_millis": 1972,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "e40c85e04019493c9e66dfc45cb22159",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": "## Configure Sweeps & Login to Weights and Biases",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "8943ce828ff2436b90600b916e7746da",
        "deepnote_cell_type": "text-cell-h2"
      }
    },
    {
      "cell_type": "code",
      "source": "sweep_config = {\n    'method': 'random',\n    'metric': {\n        'name': 'avg_reward',\n        'goal': 'maximize'   \n    },\n    'parameters': {\n        'learning_rate': {\n            'values': [0.1, 0.01, 0.001]\n        },\n        'discount': {\n            'values': [0.9, 0.95, 0.99]\n        },\n        'epsilon': {\n            'values': [0.5, 0.8, 0.9]\n        },\n        'num_states': {\n            'values': [10, 20, 30, 40]\n        },\n        'exploration_strategy': {\n            'values': ['epsilon_greedy', 'ucb']\n        },\n        'ucb_c': {\n            'values': [0.5, 1, 2]\n        },\n        'init_q_value': {\n            'values': [0, 1, 10]\n        }\n    },\n    'count': 20  # limit sweep to 20 runs\n}",
      "metadata": {
        "source_hash": "ef0aa2c4",
        "execution_start": 1687804373083,
        "execution_millis": 6,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "3e8e517a91964ddf8d7764219f244028",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": "Number of episodes to iterate",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "410f521785bf4e41b22a4015b7f1c018",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "code",
      "source": "EPISODES = 25000",
      "metadata": {
        "source_hash": "839b491d",
        "execution_start": 1687804373092,
        "execution_millis": 5,
        "deepnote_slider_step": 500,
        "deepnote_variable_name": "EPISODES",
        "deepnote_variable_value": "25000",
        "deepnote_slider_max_value": 100000,
        "deepnote_slider_min_value": 1000,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "3d464ef403e4477b85e911c371fec823",
        "deepnote_cell_type": "input-slider"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": "## Helper Functions",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "d4dd5846fb1743e9b2da41d3e9e89f46",
        "deepnote_cell_type": "text-cell-h2"
      }
    },
    {
      "cell_type": "markdown",
      "source": "choose_action_greedy - This function selects an action for a given state based on the Q-table values. It implements the ε-greedy exploration strategy, which makes a trade-off between exploration and exploitation. If a random number is greater than ε (epsilon), it chooses the action with the highest Q-value for the given state (exploitation). Otherwise, it selects a random action (exploration).\nchoose_action_UCB - This function also selects an action for a given state, but uses the Upper Confidence Bound (UCB) algorithm. This algorithm balances exploitation and exploration by choosing the action with the highest combined Q-value and exploration function value. The exploration function increases as the action is chosen less frequently, leading to a more balanced exploration of the action space.",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 20,
            "fromCodePoint": 0
          },
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 414,
            "fromCodePoint": 397
          }
        ],
        "cell_id": "7cf556bdcc0c4f5a854c08df87b24d03",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "UCB is based on the principle of optimism in the face of uncertainty, meaning it tends to prefer actions that have not been tried often. The core idea of UCB is to choose the action that has the maximum upper confidence bound, which is a sum of the current estimated value of the action and an uncertainty term. The uncertainty term increases with fewer trials of an action, thereby making less tried actions more attractive.The UCB action selection formula is as follows:",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "69caaebef2784eebbafa2e3e8411e47f",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "<img src=\"/work/SCR-20230626-rqjk.png\">Where <code>t</code> is the total number of actions taken so far, <code>Nt(a)</code> is the number of times action <code>a</code> has been taken, The number <code>c > 0</code> controls the degree of exploration.</img>\n\n\n",
      "metadata": {
        "cell_id": "55d2b1451a264b469b2da99e077ea3cb",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "update_q_table - This function updates the Q-table using the Q-Learning update rule. It calculates the new Q-value for the state-action pair as a weighted average of the old value and the learned value, where the learned value is the sum of the reward and the discounted estimate of the optimal future Q-value.",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 14,
            "fromCodePoint": 0
          }
        ],
        "cell_id": "c2c95440059b459b966f6117f0d29dbc",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "log_metrics - This function logs various metrics after each episode, including the reward of the episode, the maximum and minimum rewards obtained so far, the average reward over the last 100 episodes, and the exploration strategy used. If ε-greedy strategy is used, it also logs the current value of ε.",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 11,
            "fromCodePoint": 0
          }
        ],
        "cell_id": "8dafa709e32143639eec9b0174a9dc4d",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "code",
      "source": "def discretize_state(state, ranges):\n    discrete_state = np.zeros(len(state))\n    for i in range(len(state)):\n        discrete_state[i] = np.digitize(state[i], bins=ranges[i])\n    return tuple(discrete_state.astype(int))\n\ndef reset_environment(env, ranges):\n    observation = env.reset()\n    discrete_state = discretize_state(observation, ranges)\n    return observation, discrete_state\n\ndef choose_action_greedy(discrete_state, q_table, epsilon, env):\n    if np.random.random() > epsilon:\n        action = np.argmax(q_table[discrete_state])\n    else:\n        action = np.random.randint(0, env.action_space.n)\n    return action\n\ndef choose_action_UCB(discrete_state, q_table, env):\n    c = UCB_C\n    ucb_values = q_table[discrete_state] + c * np.sqrt(np.log(np.sum(q_table[discrete_state]) + 1) / (q_table[discrete_state] + 1e-5))\n    action = np.argmax(ucb_values)\n    return action\n\ndef update_q_table(q_table, discrete_state, action, reward, new_discrete_state, LEARNING_RATE, DISCOUNT):\n    max_future_q = np.max(q_table[new_discrete_state])  # estimate of optimal future value\n    current_q = q_table[discrete_state + (action,)]  # current Q-value\n    new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n    q_table[discrete_state + (action,)] = new_q # update Q-table with new Q-value\n\ndef log_metrics(run, reward_list, max_reward_list, min_reward_list, episode_reward, episode_steps, duration, epsilon):\n    reward_list.append(episode_reward)\n    max_reward_list.append(max(episode_reward, max_reward_list[-1]) if max_reward_list else episode_reward)\n    min_reward_list.append(min(episode_reward, min_reward_list[-1]) if min_reward_list else episode_reward)\n    avg_reward = np.mean(reward_list[-100:])  # average over last 100 episodes\n    metrics = {'eps': 1/duration, 'reward': episode_reward, 'steps': episode_steps,\n               'avg_reward': avg_reward, 'max_reward': max_reward_list[-1], \n               'min_reward': min_reward_list[-1]}\n    run.log(metrics)",
      "metadata": {
        "source_hash": "89fe6c23",
        "execution_start": 1687804373106,
        "execution_millis": 30,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "68027a95adb24caaa6e4055a3b87a446",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": "def run_episodes(run, env, q_table, ranges, epsilon, LEARNING_RATE, epsilon_decay_value, DISCOUNT, END_EPSILON_DECAYING, START_EPSILON_DECAYING, UCB_C, exploration_strategy):\n    # Additional data lists\n    reward_list = []\n    max_reward_list = []\n    min_reward_list = []\n\n    for episode in tqdm(range(EPISODES), desc=\"Training\", unit=\"episode\"):\n        start_time = time.time()\n        episode_reward = 0  # initialize the reward for this episode\n        episode_steps = 0  # initialize the number of steps for this episode\n\n        observation, discrete_state = reset_environment(env, ranges)\n        \n        done = False\n        while not done:\n            if exploration_strategy == 'ucb':\n                action = choose_action_UCB(discrete_state, q_table, env)\n            else:\n                action = choose_action_greedy(discrete_state, q_table, epsilon, env)\n            observation, reward, done, info = env.step(action)\n            new_discrete_state = discretize_state(observation, ranges)\n\n            if not done:\n                update_q_table(q_table, discrete_state, action, reward, new_discrete_state, LEARNING_RATE, DISCOUNT)\n            elif reward == 100: # Condition indicating that the lander has landed successfully\n                q_table[discrete_state + (action,)] = 0\n\n            discrete_state = new_discrete_state\n            episode_reward += reward\n            episode_steps += 1\n\n        end_time = time.time()\n        duration = end_time - start_time\n\n        log_metrics(run, reward_list, max_reward_list, min_reward_list, episode_reward, episode_steps, duration, epsilon)\n        \n        if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n            epsilon -= epsilon_decay_value",
      "metadata": {
        "source_hash": "4ad8bd50",
        "execution_start": 1687804373181,
        "execution_millis": 3,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "374e7b1610dc4de4ae07da206835b7a1",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": "def get_q_table_shape(ranges, env):\n    # Return the shape of the Q-table\n    return [len(r) - 1 for r in ranges] + [env.action_space.n]",
      "metadata": {
        "source_hash": "86aa768b",
        "execution_start": 1687804373190,
        "execution_millis": 8,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "fabe5cd801bc41f8bdb5aaffa663b733",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": "def train():\n    # Initialize a new wandb run\n    run = wandb.init(config=wandb.config, mode=\"dryrun\")\n\n    # Config is a variable that holds and saves hyperparameters and inputs\n    config = wandb.config\n\n    LEARNING_RATE = config.learning_rate\n    DISCOUNT = config.discount\n    epsilon = config.epsilon if config.exploration_strategy == 'epsilon_greedy' else None\n    START_EPSILON_DECAYING = 1\n    END_EPSILON_DECAYING = EPISODES // 2\n    epsilon_decay_value = epsilon / (END_EPSILON_DECAYING - START_EPSILON_DECAYING) if config.exploration_strategy == 'epsilon_greedy' else None\n    UCB_C = config.UCB_C if config.exploration_strategy == 'UCB' else None\n    strategy = config.exploration_strategy\n\n    env = gym.make('LunarLander-v2')\n\n    num_states_continuous = config.num_states  # The number of bins for continuous variables\n    num_states_boolean = 2  # The number of bins for boolean variables\n\n    # Define the range of each dimension\n    ranges = [\n        np.linspace(-90, 90, num_states_continuous),  # X coordinate\n        np.linspace(-90, 90, num_states_continuous),  # Y coordinate\n        np.linspace(-5, 5, num_states_continuous),  # X velocity\n        np.linspace(-5, 5, num_states_continuous),  # Y velocity\n        np.linspace(-np.pi, np.pi, num_states_continuous),  # Angle\n        np.linspace(-5, 5, num_states_continuous),  # Angular velocity\n        np.linspace(0, 1, num_states_boolean),  # Left leg contact\n        np.linspace(0, 1, num_states_boolean)  # Right leg contact\n    ]\n\n    # Initialize Q-table with the defined initial Q value\n    q_table = np.full(shape=get_q_table_shape(ranges, env), fill_value=config.init_q_value)\n    run_episodes(run, env, q_table, ranges, epsilon, LEARNING_RATE, epsilon_decay_value, DISCOUNT, END_EPSILON_DECAYING, START_EPSILON_DECAYING, UCB_C, strategy)\n\n    # Save the Q-table as an Artifact\n    artifact = wandb.Artifact('q_table', type='model')\n    np.save('q_table.npy', q_table)\n    artifact.add_file('q_table.npy')\n    run.log_artifact(artifact)\n\n    env.close()\n    run.finish() # End the Run",
      "metadata": {
        "source_hash": "f67ffa1d",
        "execution_start": 1687804373245,
        "execution_millis": 6,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "872f06bb2e184fcda8582bdde24fe5fb",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": "sweep_id = wandb.sweep(sweep_config, project=\"lunar-lander-v2\")\nwandb.agent(sweep_id, train)",
      "metadata": {
        "source_hash": "b3d59b6c",
        "execution_start": 1687804410464,
        "execution_millis": 1314,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "e7df1ab35a4b4cdcad580e6930328c8a",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'wandb' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sweep_id \u001b[38;5;241m=\u001b[39m \u001b[43mwandb\u001b[49m\u001b[38;5;241m.\u001b[39msweep(sweep_config, project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlunar-lander-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m wandb\u001b[38;5;241m.\u001b[39magent(sweep_id, train)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wandb' is not defined"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=5878bf73-13c4-4232-bd61-633eeedc1f05' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote": {},
    "orig_nbformat": 2,
    "deepnote_notebook_id": "bf000feb697b45f7a8b8a52651683634",
    "deepnote_persisted_session": {
      "createdAt": "2023-06-25T21:02:38.524Z"
    },
    "deepnote_execution_queue": []
  }
}