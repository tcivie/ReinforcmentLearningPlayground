{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Q-Learning on MountainCar-v0 Environment",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "d96e5af4e0f2403c8bebc5c3429d4a97",
        "deepnote_cell_type": "text-cell-h1"
      }
    },
    {
      "cell_type": "markdown",
      "source": "In this project, we are training a Q-Learning agent to solve the `MountainCar-v0` environment from OpenAI's Gym. ",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "491ec8ad01714bd9a1d554aa0e3027f6",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "The objective of the MountainCar environment is to get an underpowered car to the top of a hill. The car is on a one-dimensional track, and the position and velocity of the car are observable at each time step.",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "b8585933104942bda4aba4570ca5bdd0",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Training Process",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "cfc13fe739824c54b3005b8f2235041f",
        "deepnote_cell_type": "text-cell-h2"
      }
    },
    {
      "cell_type": "markdown",
      "source": "We implement a Q-Learning algorithm with an ε-greedy policy for action selection. We use a simple table to represent the Q-values of state-action pairs. To handle the continuous state space of the environment, we discretize the states by splitting the position and velocity into bins.",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "12f2f3892afd47e2b853b1436861ccb5",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "The agent's goal is to maximize the total reward it receives in an episode. The reward at each time step is -1, and an episode ends when the car reaches the goal (position 0.5) or after 200 time steps.",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "78af1a3e2aa04e55ba4a86d45e93d775",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Hyperparameter Tuning",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "ed96c73f79a9450abee83ea9d365719d",
        "deepnote_cell_type": "text-cell-h2"
      }
    },
    {
      "cell_type": "markdown",
      "source": "We use Weights & Biases Sweeps for hyperparameter tuning. We explore different values of the learning rate, discount factor, and the number of discretized states. The agent's performance is measured by the average reward over 100 episodes.",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "0e5df5d6d0984d54a9d2d819caa8627d",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Analysis and Visualization",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "89364b7a8252424a99a670b74d2730f8",
        "deepnote_cell_type": "text-cell-h2"
      }
    },
    {
      "cell_type": "markdown",
      "source": "We use Weights & Biases for experiment tracking and visualization. We log the following metrics during training:",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "28d95bb7b2ec4f53895e00e6d07afcd4",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "- Episode Reward: The total reward obtained in an episode.\n- Steps: The number of steps taken in an episode.\n- Epsilon: The current value of ε in the ε-greedy policy.\n- Average Reward: The average episode reward over the last 100 episodes.\n- Max/Min Reward: The maximum/minimum episode reward obtained so far.",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "84d7393cb0134462b75dc739717b8693",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "The results are visualized on a Weights & Biases dashboard, which shows how the agent's performance evolves over time as it learns from its interactions with the environment.",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "5336368b456d4c8c91b38e8d2e1878c1",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## Results",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "4e532fee11e544adb763443ab9ba0806",
        "deepnote_cell_type": "text-cell-h2"
      }
    },
    {
      "cell_type": "markdown",
      "source": "After training for a specified number of episodes, the agent is able to consistently reach the goal within the 200 time step limit. The hyperparameters found by the sweep lead to more efficient learning compared to a manually chosen baseline.",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "28a5c0ed3675457eb9decc4a472c2682",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "This project demonstrates the effectiveness of Q-Learning and the importance of hyperparameter tuning for reinforcement learning tasks. The next steps could include experimenting with other RL algorithms or environments.\n",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "52ea596958724e2196ffcae6983969fb",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "# Code and running",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "70c521d857614ccf931ebae46992a473",
        "deepnote_cell_type": "text-cell-h1"
      }
    },
    {
      "cell_type": "code",
      "source": "!pip install --upgrade wandb\n!pip install gym==0.26.2\n!pip install wandb",
      "metadata": {
        "source_hash": "8607f5f7",
        "execution_start": 1687340778078,
        "execution_millis": 32072,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "50f10faa1726465fb9cb6c0f33127e96",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting wandb\n  Downloading wandb-0.15.4-py3-none-any.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions in /shared-libs/python3.9/py/lib/python3.9/site-packages (from wandb) (4.4.0)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from wandb) (1.24.0)\nCollecting GitPython!=3.1.29,>=1.0.0\n  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from wandb) (2.28.1)\nCollecting PyYAML\n  Downloading PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.8/661.8 KB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pathtools\n  Downloading pathtools-0.1.2.tar.gz (11 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting setproctitle\n  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\nCollecting docker-pycreds>=0.4.0\n  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\nCollecting appdirs>=1.4.3\n  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\nRequirement already satisfied: psutil>=5.0.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from wandb) (8.1.3)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from wandb) (3.19.6)\nRequirement already satisfied: setuptools in /root/venv/lib/python3.9/site-packages (from wandb) (58.1.0)\nRequirement already satisfied: six>=1.4.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nCollecting gitdb<5,>=4.0.1\n  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\nRequirement already satisfied: charset-normalizer<3,>=2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\nCollecting smmap<6,>=3.0.1\n  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\nBuilding wheels for collected packages: pathtools\n  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8808 sha256=46a28638004c68825f7a76aec39cf11c14fc2e41931e2b4ee622237b5e3256e4\n  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\nSuccessfully built pathtools\nInstalling collected packages: pathtools, appdirs, smmap, setproctitle, PyYAML, docker-pycreds, gitdb, GitPython, wandb\nSuccessfully installed GitPython-3.1.31 PyYAML-6.0 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 setproctitle-1.3.2 smmap-5.0.0 wandb-0.15.4\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mCollecting gym==0.26.2\n  Downloading gym-0.26.2.tar.gz (721 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 KB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting gym-notices>=0.0.4\n  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\nRequirement already satisfied: numpy>=1.18.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from gym==0.26.2) (1.23.4)\nCollecting cloudpickle>=1.2.0\n  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\nRequirement already satisfied: importlib-metadata>=4.8.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from gym==0.26.2) (5.0.0)\nRequirement already satisfied: zipp>=0.5 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gym==0.26.2) (3.9.0)\nBuilding wheels for collected packages: gym\n  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827650 sha256=49c80f6ff1201ba39f62c919b2c7ee7bac5450e4a85487f00dbd2b99e4ab0fc9\n  Stored in directory: /root/.cache/pip/wheels/af/2b/30/5e78b8b9599f2a2286a582b8da80594f654bf0e18d825a4405\nSuccessfully built gym\nInstalling collected packages: gym-notices, cloudpickle, gym\nSuccessfully installed cloudpickle-2.2.1 gym-0.26.2 gym-notices-0.0.8\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: wandb in /root/venv/lib/python3.9/site-packages (0.15.4)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from wandb) (8.1.3)\nRequirement already satisfied: psutil>=5.0.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: PyYAML in /root/venv/lib/python3.9/site-packages (from wandb) (6.0)\nRequirement already satisfied: pathtools in /root/venv/lib/python3.9/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from wandb) (2.28.1)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from wandb) (1.24.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /root/venv/lib/python3.9/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: typing-extensions in /shared-libs/python3.9/py/lib/python3.9/site-packages (from wandb) (4.4.0)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from wandb) (3.19.6)\nRequirement already satisfied: appdirs>=1.4.3 in /root/venv/lib/python3.9/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: setproctitle in /root/venv/lib/python3.9/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /root/venv/lib/python3.9/site-packages (from wandb) (3.1.31)\nRequirement already satisfied: setuptools in /root/venv/lib/python3.9/site-packages (from wandb) (58.1.0)\nRequirement already satisfied: six>=1.4.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /root/venv/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\nRequirement already satisfied: idna<4,>=2.5 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /root/venv/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m",
          "output_type": "stream"
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "Import necessary libraries",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "cfedd4fa44ff43fb8d79ec625f388c28",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "code",
      "source": "import gym\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport wandb\nimport time\nimport os",
      "metadata": {
        "source_hash": "3e647315",
        "execution_start": 1687340810164,
        "execution_millis": 1447,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "68e5300d37934c4db3686c708f4e80c6",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": "## Configure Sweeps & Login to Weights and Biases",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "a3111123f2914acc8afe3096fbcfa614",
        "deepnote_cell_type": "text-cell-h2"
      }
    },
    {
      "cell_type": "code",
      "source": "wandb.login()\nsweep_config = {\n    'method': 'random',  # or 'grid' or 'bayes'\n    'metric': {\n        'name': 'avg_reward',\n        'goal': 'maximize'   \n    },\n    'parameters': {\n        'learning_rate': {\n            'values': [0.1, 0.01, 0.001]\n        },\n        'discount': {\n            'values': [0.9, 0.95, 0.99]\n        },\n        'epsilon': {\n            'values': [0.5, 0.8, 0.9]\n        },\n        'num_states': {\n            'values': [10, 20, 30, 40]\n        },\n    },\n    'count': 20  # limit sweep to 20 runs\n}\n\nsweep_id = wandb.sweep(sweep_config, project=\"mountain-car-v0\")",
      "metadata": {
        "source_hash": "6cea42bb",
        "execution_start": 1687340811631,
        "execution_millis": 3509,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "8bd3736460ef490f818a956287f97a2a",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtcivie\u001b[0m (\u001b[33mgot-tree\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m To avoid this, please fix the sweep config schema violations below:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 1. Additional properties are not allowed ('count' was unexpected)\nCreate sweep with ID: lf1ym46s\nSweep URL: https://wandb.ai/got-tree/mountain-car-v0/sweeps/lf1ym46s\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": "Number of episodes to iterate",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "d341fcd216194740847197274ec7a8ae",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "code",
      "source": "EPISODES = 25000",
      "metadata": {
        "source_hash": "839b491d",
        "execution_start": 1687340815145,
        "execution_millis": 4,
        "deepnote_slider_step": 500,
        "deepnote_variable_name": "EPISODES",
        "deepnote_variable_value": "25000",
        "deepnote_slider_max_value": 100000,
        "deepnote_slider_min_value": 1000,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "bef3af04ecd64a888e90b27aed476e8c",
        "deepnote_cell_type": "input-slider"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": "This function takes in a continuous state and returns a discrete state",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "81b3fa4bd7184d8a813462bebcce6355",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "code",
      "source": "def get_discrete_state(state, env, bin_size):\n    # This is used to convert continuous state space into a discrete state space\n    discrete_state = (state - env.observation_space.low) / bin_size\n    return tuple(discrete_state.astype(int))",
      "metadata": {
        "source_hash": "bfaca051",
        "execution_start": 1687340815151,
        "execution_millis": 3,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "22b5fe2d32694345b280d83d2fab5cec",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": "## Defining the main training loop",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "80fd0a5b6a6a41548a6eeadc64cf24a1",
        "deepnote_cell_type": "text-cell-h2"
      }
    },
    {
      "cell_type": "markdown",
      "source": "The main loop for the Q-learning algorithm is in the section where we loop over each episode. Inside this loop:",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "5cab67d9bb36479b8d720f592840f040",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "- We reset the environment and initialize the reward and steps for this episode to zero.",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "337a9e78f9eb4f31ab2dbc575bd28ddb",
        "deepnote_cell_type": "text-cell-bullet"
      }
    },
    {
      "cell_type": "markdown",
      "source": "- We use the epsilon-greedy method to select actions and execute them in the environment.",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "7b5dd8cca1404666ab24295dfe500d1e",
        "deepnote_cell_type": "text-cell-bullet"
      }
    },
    {
      "cell_type": "markdown",
      "source": "- The Q-value for the executed action is updated using the Q-learning update rule.We also log the episode per second, reward, reward per second, and steps per second to wandb.",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "eae605e9a574427385bc54361e79c825",
        "deepnote_cell_type": "text-cell-bullet"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Creating helper functions",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "29d5ec04be49400a9da94763f0ef9f7e",
        "deepnote_cell_type": "text-cell-h3"
      }
    },
    {
      "cell_type": "markdown",
      "source": "reset_environment(env, bin_size) - This function resets the environment to its initial state at the beginning of each episode. It also converts the initial state into a discrete format, as our Q-table is based on discrete states and actions. It takes the environment and bin size as input and returns the initial observation and the discrete state.",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 32,
            "fromCodePoint": 0
          }
        ],
        "cell_id": "7fa062d5d33b45a487d93453b1ba131e",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "choose_action(discrete_state, q_table, epsilon, env) - This function implements the ε-greedy policy for action selection. With a probability of ε, it selects a random action, and with a probability of (1-ε), it selects the action with the highest Q-value in the current state. It takes the current discrete state, the Q-table, the epsilon value, and the environment as input and returns the chosen action.",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 52,
            "fromCodePoint": 0
          }
        ],
        "cell_id": "04dd4add92e745f6af49250e39d7ee01",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "update_q_table(q_table, discrete_state, action, reward, new_discrete_state, LEARNING_RATE, DISCOUNT) - This function updates the Q-value for the current state-action pair based on the Q-Learning update rule. It takes the Q-table, the current discrete state, the chosen action, the reward obtained, the new discrete state, learning rate, and discount factor as input. It doesn't return anything as the Q-table is updated in-place.",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 100,
            "fromCodePoint": 0
          }
        ],
        "cell_id": "f73f1a21b2f14d32aa670a9357a1ad5b",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "This is the Q-Learning formula used in the <i>update_q_table</i> function:<image src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/d247db9eaad4bd343e7882ec546bf3847ebd36d8\"></image><ul>\n<li>Q(s,a) is the current estimate of the Q-value for the state-action pair (s, a)</li><li>α is the learning rate</li><li>r is the immediate reward obtained after taking action a in state s</li><li>γ is the discount factor</li><li>max Q(s',a') is the maximum Q-value over all actions a' in the next state s'</li></ul>",
      "metadata": {
        "cell_id": "8a6ed717848644d89ac8634252448c49",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "log_metrics(run, reward_list, max_reward_list, min_reward_list, episode_reward, episode_steps, duration, epsilon) - This function logs various metrics of interest during the training process. These metrics include the average reward over the last 100 episodes, the total reward in the current episode, the number of steps in the current episode, the current ε value, and the minimum and maximum rewards obtained so far. It takes the current run, lists to store total, maximum and minimum rewards per episode, reward for the current episode, number of steps in the current episode, duration of the current episode, and the current ε value as input. The metrics are logged to the current Weights & Biases run for visualizing the training progress.",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 113,
            "fromCodePoint": 0
          }
        ],
        "cell_id": "eedc209f92484e85b788abd2bce2ba0b",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "95373846fbe64760b05546877e8ac5ef",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "code",
      "source": "def reset_environment(env, bin_size):\n    observation, info = env.reset()\n    discrete_state = get_discrete_state(observation, env, bin_size)\n    return observation, discrete_state\n\ndef choose_action(discrete_state, q_table, epsilon, env):\n    if np.random.random() > epsilon:\n        action = np.argmax(q_table[discrete_state])\n    else:\n        action = np.random.randint(0, env.action_space.n)\n    return action\n\ndef update_q_table(q_table, discrete_state, action, reward, new_discrete_state, LEARNING_RATE, DISCOUNT):\n    max_future_q = np.max(q_table[new_discrete_state])  # estimate of optimal future value\n    current_q = q_table[discrete_state + (action,)]  # current Q-value\n    new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n    q_table[discrete_state + (action,)] = new_q # update Q-table with new Q-value\n\ndef log_metrics(run, reward_list, max_reward_list, min_reward_list, episode_reward, episode_steps, duration, epsilon):\n    reward_list.append(episode_reward)\n    max_reward_list.append(max(episode_reward, max_reward_list[-1]) if max_reward_list else episode_reward)\n    min_reward_list.append(min(episode_reward, min_reward_list[-1]) if min_reward_list else episode_reward)\n    avg_reward = np.mean(reward_list[-100:])  # average over last 100 episodes\n    metrics = {'eps': 1/duration, 'reward': episode_reward, 'steps': episode_steps,\n               'epsilon': epsilon, 'avg_reward': avg_reward, 'max_reward': max_reward_list[-1], \n               'min_reward': min_reward_list[-1]}\n    run.log(metrics)\n",
      "metadata": {
        "source_hash": "d4c13a9b",
        "execution_start": 1687340815160,
        "execution_millis": 8,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "e9c4b8d2005940248b8d40a7840ee4d7",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": "### The main loop",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "0f4d76b367da4b86b3d7292c40d8ad8c",
        "deepnote_cell_type": "text-cell-h3"
      }
    },
    {
      "cell_type": "markdown",
      "source": "run_episodes(run, env, q_table, bin_size, epsilon, LEARNING_RATE, epsilon_decay_value, DISCOUNT, END_EPSILON_DECAYING, START_EPSILON_DECAYING) - This function contains the main training loop. In each episode, it resets the environment, selects actions according to the ε-greedy policy, takes the actions in the environment, updates the Q-table, and logs the training metrics. It takes the current run, the environment, the Q-table, the bin size for discretizing states, the initial epsilon value, the learning rate, the epsilon decay value, the discount factor, and the start and end episodes for epsilon decay as input. The Q-table gets updated in-place during the training process, and the training metrics are logged to the current Weights & Biases run.",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 142,
            "fromCodePoint": 0
          },
          {
            "type": "marks",
            "marks": {
              "bold": true
            },
            "toCodePoint": 190,
            "fromCodePoint": 168
          }
        ],
        "cell_id": "87a2ded52b024fd9b02cfbed5bfbf20c",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "code",
      "source": "def run_episodes(run, env, q_table, bin_size, epsilon, LEARNING_RATE, epsilon_decay_value, DISCOUNT, END_EPSILON_DECAYING, START_EPSILON_DECAYING):\n    # Additional data lists\n    reward_list = []\n    max_reward_list = []\n    min_reward_list = []\n    \n    for episode in tqdm(range(EPISODES), desc=\"Training\", unit=\"episode\"):\n        start_time = time.time()\n        episode_reward = 0  # initialize the reward for this episode\n        episode_steps = 0  # initialize the number of steps for this episode\n\n        observation, discrete_state = reset_environment(env, bin_size)\n        \n        done = False\n        while not done:\n            action = choose_action(discrete_state, q_table, epsilon, env)\n            observation, reward, terminated, truncated, info = env.step(action)\n            new_discrete_state = get_discrete_state(observation, env, bin_size)\n\n            if not done:\n                update_q_table(q_table, discrete_state, action, reward, new_discrete_state, LEARNING_RATE, DISCOUNT)\n            if observation[0] >= env.goal_position:\n                done = True\n                q_table[discrete_state + (action,)] = 0\n\n            discrete_state = new_discrete_state\n            episode_reward += reward\n            episode_steps += 1\n\n        end_time = time.time()\n        duration = end_time - start_time\n\n        log_metrics(run, reward_list, max_reward_list, min_reward_list, episode_reward, episode_steps, duration, epsilon)\n        \n        if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n            epsilon -= epsilon_decay_value",
      "metadata": {
        "source_hash": "9c6543a9",
        "execution_start": 1687340815172,
        "execution_millis": 43,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "8f9892bdf6f2416289a2ab55449852df",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": "train() is a warper for the run_episodes() which is made to utilise the Weights and Biases sweeps functionality",
      "metadata": {
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 7,
            "fromCodePoint": 0
          },
          {
            "type": "marks",
            "marks": {
              "code": true
            },
            "toCodePoint": 42,
            "fromCodePoint": 28
          }
        ],
        "cell_id": "75e3b78b18cb4eb38694ba997dabbd83",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "code",
      "source": "def train():\n\n    # Initialize a new wandb run\n    run = wandb.init(config=wandb.config)\n\n    # Config is a variable that holds and saves hyperparameters and inputs\n    config = wandb.config\n\n    LEARNING_RATE = config.learning_rate\n    DISCOUNT = config.discount\n    epsilon = config.epsilon\n    START_EPSILON_DECAYING = 1\n    END_EPSILON_DECAYING = EPISODES // 2\n    num_states = np.array([config.num_states, config.num_states])\n    epsilon_decay_value = epsilon / (END_EPSILON_DECAYING - START_EPSILON_DECAYING)\n\n    env = gym.make('MountainCar-v0')\n    bin_size = (env.observation_space.high - env.observation_space.low) / num_states\n\n    # Initialize Q-table with zeros\n    q_table = np.zeros(shape=(num_states[0] ,num_states[1], env.action_space.n))\n    run_episodes(run, env, q_table, bin_size, epsilon, LEARNING_RATE, epsilon_decay_value, DISCOUNT, END_EPSILON_DECAYING, START_EPSILON_DECAYING)\n    env.close()\n    run.finish()  # End the run",
      "metadata": {
        "source_hash": "4a339ba6",
        "execution_start": 1687340815214,
        "execution_millis": 2,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "23021fff98c54305b92766d86e23718a",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": "Run the sweeps",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "cd40ae8d645a463d99471a3af5c49856",
        "deepnote_cell_type": "text-cell-p"
      }
    },
    {
      "cell_type": "code",
      "source": "sweep_id = wandb.sweep(sweep_config, project=\"mountain-car-v0\")\nwandb.agent(sweep_id, train)\n",
      "metadata": {
        "source_hash": "5920a4fe",
        "execution_start": 1687340815257,
        "execution_millis": 336441,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "c9575498076b494e97993f177435f54b",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m To avoid this, please fix the sweep config schema violations below:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 1. Additional properties are not allowed ('count' was unexpected)\nCreate sweep with ID: v31n9m5m\nSweep URL: https://wandb.ai/got-tree/mountain-car-v0/sweeps/v31n9m5m\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: n7gmrlk2 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdiscount: 0.95\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon: 0.9\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_states: 10\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=5878bf73-13c4-4232-bd61-633eeedc1f05' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote": {},
    "orig_nbformat": 2,
    "deepnote_notebook_id": "b873c066d23d42898e0265d6618dd70d",
    "deepnote_persisted_session": {
      "createdAt": "2023-06-20T21:05:26.999Z"
    },
    "deepnote_execution_queue": []
  }
}