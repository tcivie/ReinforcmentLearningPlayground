# ReinforcementLearningPlayground
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![NumPy](https://img.shields.io/badge/numpy-013243?style=for-the-badge&logo=numpy&logoColor=white)
![OpenAI Gym](https://img.shields.io/badge/OpenAI%20Gym-0081A7?style=for-the-badge&logo=openai&logoColor=white)
![W&B](https://img.shields.io/badge/Weights%20&%20Biases-FE7A16?style=for-the-badge&logo=weights-and-biases&logoColor=white)

Welcome to my Reinforcement Learning Playground! üéâ This repository serves as my hands-on lab üß™ to explore, understand, and implement reinforcement learning algorithms.

## Project Description üìã

The goal of this project is to create a comprehensive guide üìö to understanding and building reinforcement learning algorithms. The algorithms are demonstrated in the context of the The Farama Foundation's Gym, previously known as OpenAI's Gym. Gym provides a wide variety of environments to test and compare these algorithms' performances.

In this project, we focus on Q-Learning, a simple yet powerful value-based reinforcement learning algorithm. The entire learning process is captured and optimized using Weights and Biases, a tool that allows tracking and visualizing metrics üìà during the training process. This helps in understanding the learning dynamics and optimizing the hyperparameters for the best performance.

## Key Features üîë

Implementation of Q-Learning algorithm: A value-based RL algorithm that learns the optimal policy for decision-making problems.
Hyperparameter tuning: Utilizing Weights and Biases' sweep feature, various hyperparameters for the learning algorithm are experimented with to find the best performing ones.
Performance Metrics: Tracks and visualizes several key performance metrics such as reward per episode, steps per episode, and epsilon (for Œµ-greedy policy) over the course of learning.
Code modularity: The code is divided into multiple functions for better readability and easy debugging.

## How to use üö¶

This project is built and tested in Deepnote, a powerful online Jupyter notebook platform. To run this notebook:

1. Clone the repository to your local machine.
2. Upload the notebook to Deepnote.
3. Ensure that you have the necessary libraries installed (check the import statements at the top of the notebook).
4. Run the cells to train the Q-Learning algorithm.
5. Monitor the training progress and results in the Weights and Biases dashboard.

## 
I invite you to explore this playground and hope it serves as a useful resource in your own reinforcement learning journey! Your suggestions and contributions to improve this guide are most welcome. Enjoy learning! üéì

# Deepnote Project Links üåê

Find detailed explanations, insights, and more in the accompanying Deepnote articles:

| Project | Code and Run | Data Report |
|---|---|---|
|Q-Learning on MountainCar-v0 Environment|[Code](https://deepnote.com/workspace/sce-63d4-fc7602c2-ea3a-4733-a945-73a5fb5cde5d/project/Reinforcment-Learning-Playground-5878bf73-13c4-4232-bd61-633eeedc1f05/notebook/MountainCar-b873c066d23d42898e0265d6618dd70d)|[Report](https://api.wandb.ai/links/got-tree/ja3o7fb0)|
|Q-Learning on LunarLander-v2 Environment|[Code](https://deepnote.com/workspace/sce-63d4-fc7602c2-ea3a-4733-a945-73a5fb5cde5d/project/Reinforcment-Learning-Playground-5878bf73-13c4-4232-bd61-633eeedc1f05/notebook/LunarLander-bf000feb697b45f7a8b8a52651683634)|[Report](https://api.wandb.ai/links/got-tree/0tw2h06o)|

(Additional links will be added here as the articles are created)
